{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook is for practicing XGBoost and initial analyzation of Tidal Benthic dataset from the [Chesapeake Bay Project DataHub](https://datahub.chesapeakebay.net/LivingResources). This data goes through September 26, 2013. \n",
    "\n",
    "The Tidal Benthic database measures sediment, bio mass data, taxonomic data, and water quality data. These separate datasets were merged using the monitoring event data, as decribed by the [2012 Users Guide to CBP Biological Monitoring Data](https://d18lev1ok5leia.cloudfront.net/chesapeakebay/documents/guide2012_final.pdf). More cleaning is described in the [cleaning notebook](../../notebooks/planton_patrol-APIs/plank_cleaningBenthic.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And read in our parameter dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Import from JSON file\n",
    "with open('../../data/plank_CBPparam_dict.json', 'r') as f:\n",
    "    benthic_param_dict = json.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the 'wide' dataset\n",
    "\n",
    "The 'wide' dataset converts each parameter to a column, creating one row per `FieldActivityId`. That is, any data with the same location (including depth), data, time, and sample volume will be in the same row of the dataframe. There are 903 data points from July 29, 2004 throught September 26, 2013."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "benthic_data_wide = pd.read_csv('../../data/plank_ChesapeakeBayBenthic_clean_wide.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, a few columns somehow survived cleaning. Let's remove them again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "benthic_data_wide = benthic_data_wide.drop(columns=['EventId','SampleType','SampleReplicate']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(903, 569)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "benthic_data_wide.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also verify that there are 903 unique samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "903"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "benthic_data_wide['FieldActivityId'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since `FieldActitivyId` is a unique numerical value for each sample, it might give false correlations. Let's remove it (others should probably be removed later). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "benthic_data_wide = benthic_data_wide.drop(columns='FieldActivityId')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lot of parameters are missing values, especially the taxonomic and sediment data. Let's remove columns missing more than half of the values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining columns: Index(['CBSeg2003', 'CBSeg2003Description', 'Station', 'Latitude', 'Longitude',\n",
      "       'SampleDate', 'SampleTime', 'TotalDepth', 'Source', 'ProjectIdentifier',\n",
      "       'SampleVolume', 'PDepth', 'Salzone', 'MOIST', 'SAND', 'SILTCLAY', 'TC',\n",
      "       'TIC', 'TN', 'TOC', 'PCT_CARN_OMN', 'PCT_DEPO', 'PCT_PI_ABUND',\n",
      "       'PCT_PI_BIO', 'PCT_PS_ABUND', 'PCT_PS_BIO', 'SW', 'TOTAL_SCORE',\n",
      "       'TOT_ABUND', 'TOT_BIOMASS_G', 'SampleDepth', 'DO', 'DO_SAT_P', 'PH',\n",
      "       'SALINITY', 'SPCOND', 'WTEMP'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.5\n",
    "\n",
    "missing_percentage = benthic_data_wide.isnull().mean()\n",
    "\n",
    "columns_to_keep = missing_percentage[missing_percentage <= threshold].index\n",
    "\n",
    "# Create a new DataFrame with only the columns to keep\n",
    "benthic_data_less_wide = benthic_data_wide[columns_to_keep]\n",
    "\n",
    "print('Remaining columns:', benthic_data_less_wide.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Updating the dictionary--first time only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of these columns are already in our parameter dictionary, and some do not need to be (SampleDate, Latitude, Longitude). Let's find which ones are missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_in_data = benthic_data_less_wide.columns.to_list()\n",
    "\n",
    "no_dict_columns = ['CBSeg2003', 'CBSeg2003Description', 'Station', 'Latitude', 'Longitude', 'SampleDate', 'SampleTime', 'TotalDepth', 'Source','ProjectIdentifier', 'SampleVolume', 'PDepth', 'Salzone']\n",
    "\n",
    "columns_to_check = [value for value in columns_in_data if value not in no_dict_columns]\n",
    "\n",
    "[col for col in columns_to_check if col not in benthic_param_dict]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we consult the [ 2012 Users Guide to CBP Biological Monitoring Data](https://d18lev1ok5leia.cloudfront.net/chesapeakebay/documents/guide2012_final.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_param_dict = {'PCT_CARN_OMN': {'Units': 'Percent', 'Type':'Percent Carnivores And Omnivores'},\n",
    " 'PCT_DEPO': {'Units': 'Percent', 'Type':'Percent Deep Deposit Feeders'},\n",
    " 'PCT_PI_ABUND': {'Units': 'Percent', 'Type': 'Percent Pollution Indicative Species Abundance'},\n",
    " 'PCT_PI_BIO': {'Units': 'Percent', 'Type': 'Percent Pollution Indicative Species Biomass'},\n",
    " 'PCT_PS_ABUND': {'Units': 'Percent', 'Type': 'Percent Pollution Sensitive Species Abundance-Tidal Benthic'},\n",
    " 'PCT_PS_BIO': {'Units': 'Percent', 'Type': 'Percent Pollution Sensitive Species Biomass'},\n",
    " 'SW': {'Units': 'None', 'Type': 'Shannon Wiener Index'},\n",
    " 'TOTAL_SCORE': {'Units': 'None', 'Type': 'Total Benthic Restoration Goal Score For Sample'},\n",
    " 'TOT_ABUND': {'Units': 'Count', 'Type': 'Total Number Of Individuals'},\n",
    " 'TOT_BIOMASS_G': {'Units': 'Will vary', 'Type': 'Total Species Biomass In'},\n",
    " 'SampleDepth':  {'Units': 'M', 'Type': 'Sample Collection DepthTotal Species Biomass In'}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # only run once!\n",
    "# new_param_dict = {**benthic_param_dict, **additional_param_dict}\n",
    "\n",
    "# with open('../../data/plank_CBPparam_dict.json', 'w') as f:\n",
    "#     json.dump(new_param_dict, f)\n",
    "\n",
    "# # Reimport from JSON file\n",
    "# with open('../../data/plank_CBPparam_dict.json', 'r') as f:\n",
    "#     benthic_param_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this dataset did not include any chlrophyl data, we will pick some other parameters to model. How about `PCT_PI_BIO` - Percent Pollution Indicative Species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a list of categorical data columns for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['CBSeg2003', 'CBSeg2003Description', 'Station', 'Latitude', 'Longitude',\n",
       "       'SampleDate', 'SampleTime', 'TotalDepth', 'Source', 'ProjectIdentifier',\n",
       "       'SampleVolume', 'PDepth', 'Salzone', 'MOIST', 'SAND', 'SILTCLAY', 'TC',\n",
       "       'TIC', 'TN', 'TOC', 'PCT_CARN_OMN', 'PCT_DEPO', 'PCT_PI_ABUND',\n",
       "       'PCT_PI_BIO', 'PCT_PS_ABUND', 'PCT_PS_BIO', 'SW', 'TOTAL_SCORE',\n",
       "       'TOT_ABUND', 'TOT_BIOMASS_G', 'SampleDepth', 'DO', 'DO_SAT_P', 'PH',\n",
       "       'SALINITY', 'SPCOND', 'WTEMP'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "benthic_data_less_wide.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rl/kqt6tbv90l9_pwc4927vdb340000gn/T/ipykernel_76765/2677993439.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  benthic_data_less_wide[col] = benthic_data_less_wide[col].astype('category')\n"
     ]
    }
   ],
   "source": [
    "# List of categorical columns\n",
    "cat_cols = ['CBSeg2003', 'CBSeg2003Description', 'Station', 'Source', 'ProjectIdentifier', 'Salzone']\n",
    "\n",
    "# Convert categorical columns to category type\n",
    "for col in cat_cols:\n",
    "    benthic_data_less_wide[col] = benthic_data_less_wide[col].astype('category')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
